# -*- coding: utf-8 -*-
"""Heart Failure Prediction & Model Comparison

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bQAG-2TE57gRA3ZETTw8bb-gT-oIE_g_
"""

# =============================
# Heart Failure Prediction
# Models: Logistic Regression, Decision Tree, Random Forest
# =============================

# 1. Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import ( accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, ConfusionMatrixDisplay, RocCurveDisplay )

# 2. Load Dataset
url= 'https://raw.githubusercontent.com/Kaju-barnwal/DataSets/refs/heads/main/heart_failure_clinical_records_dataset.csv'
pd.read_csv(url)
print(df)

# OUTPUT=DEATH_EVENT
# INPUT = age,anaemia,creatinine_phosphokinase,diabetes,ejection_fraction,high_blood_pressure,platelets,serum_creatinine,serum_sodium,sex,smoking,time

# Basic info about dataset
print("Dataset Shape:", df.shape)      # rows, columns
print(df.info())                       # column datatypes + missing values

print("\nTarget Class Distribution (0=Alive, 1=Died):")
print(df['DEATH_EVENT'].value_counts())  # how many survived vs died

# 3. Split into Input (X) and Output (y)
X = df.drop('DEATH_EVENT', axis=1)  # all features
y = df['DEATH_EVENT']               # target column

# 4. Train-Test Split (70% training, 30% testing)
# stratify=y → ensures same proportion of died/alive in train & test
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=42, stratify=y )

print(y.shape) #100%
print(y_train.shape) #75%
print(y_test.shape) #25%

# 5. Scaling (ONLY for Logistic Regression, not trees)
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)  # fit + transform on train
X_test_scaled = scaler.transform(X_test)        # only transform on test

# =============================
# Logistic Regression
# =============================
# class_weight='balanced' → handles class imbalance
log_model = LogisticRegression(solver='liblinear', class_weight='balanced', random_state=42)
log_model.fit(X_train_scaled, y_train)
y_pred_log = log_model.predict(X_test_scaled)

# =============================
# Decision Tree
# =============================
# max_depth=5 → prevents overfitting (can tune further)
tree_model = DecisionTreeClassifier(max_depth=5, random_state=42)
tree_model.fit(X_train, y_train)
y_pred_tree = tree_model.predict(X_test)

# =============================
# Random Forest
# =============================
# n_estimators=100 → 100 trees, max_depth=7 → controlled complexity
rf_model = RandomForestClassifier(n_estimators=100, max_depth=7, random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

# =============================
# Evaluation Function (Reusable)
# =============================
def evaluate_model(name, y_test, y_pred, model, X_test):
    """
    This function calculates Accuracy, Precision, Recall, F1, ROC-AUC,
    prints classification report + confusion matrix,
    and returns results for comparison.
    """
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc = roc_auc_score(y_test, model.predict_proba(X_test)[:,1])  # AUC based on probabilities

    print(f"\n{name} Performance:")
    print(classification_report(y_test, y_pred))   # precision, recall, f1 per class
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))  # TP/TN/FP/FN

    return [name, acc, prec, rec, f1, auc]

# =============================
# Compare All Models
# =============================
results = []
results.append(evaluate_model("Logistic Regression", y_test, y_pred_log, log_model, X_test_scaled))
results.append(evaluate_model("Decision Tree", y_test, y_pred_tree, tree_model, X_test))
results.append(evaluate_model("Random Forest", y_test, y_pred_rf, rf_model, X_test))

# Final Comparison Table
comparison_df = pd.DataFrame(results, columns=["Model", "Accuracy", "Precision", "Recall", "F1 Score", "ROC-AUC"])
print("\n Final Model Comparison ")
print(comparison_df)

# Plot all confusion matrices side by side
fig, axes = plt.subplots(1, 3, figsize=(15,4))
models = { "Logistic Regression": y_pred_log,
    "Decision Tree": y_pred_tree,
    "Random Forest": y_pred_rf }

for ax, (name, y_pred) in zip(axes, models.items()):
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap="Blues", cbar=False,
                xticklabels=["Alive (0)", "Died (1)"],
                yticklabels=["Alive (0)", "Died (1)"],
                ax=ax)
    ax.set_title(f"{name}")
    ax.set_xlabel("Predicted")
    ax.set_ylabel("Actual")

plt.tight_layout()
plt.show()

plt.figure(figsize=(7,6))

# Plot ROC curves for all models
RocCurveDisplay.from_estimator(log_model, X_test_scaled, y_test, name="Logistic Regression", ax=plt.gca())
RocCurveDisplay.from_estimator(tree_model, X_test, y_test, name="Decision Tree", ax=plt.gca())
RocCurveDisplay.from_estimator(rf_model, X_test, y_test, name="Random Forest", ax=plt.gca())

# Diagonal line for random guessing
plt.plot([0,1], [0,1], 'k--', label="Random Guess")

# Title & legend
plt.title("ROC Curve Comparison")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc="lower right")
plt.show()